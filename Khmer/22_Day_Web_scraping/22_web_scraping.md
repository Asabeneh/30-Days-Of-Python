<div align="center">
  <h1> 30 Days Of Python: Day 22 - Web Scraping </h1>
  <a class="header-badge" target="_blank" href="https://www.linkedin.com/in/asabeneh/">
  <img src="https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social">
  </a>
  <a class="header-badge" target="_blank" href="https://twitter.com/Asabeneh">
  <img alt="Twitter Follow" src="https://img.shields.io/twitter/follow/asabeneh?style=social">
  </a>

<sub>Author:
<a href="https://www.linkedin.com/in/asabeneh/" target="_blank">Asabeneh Yetayeh</a><br>
<small> Second Edition: July, 2021</small>
</sub>
</div>

[<< Day 21](../21_Day_Classes_and_objects/21_classes_and_objects.md) | [Day 23 >>](../23_Day_Virtual_environment/23_virtual_environment.md)

![30DaysOfPython](../images/30DaysOfPython_banner3@2x.png)

- [ğŸ“˜ Day 22](#-day-22)
  - [Python Web Scraping](#python-web-scraping)
    - [What is Web Scrapping](#what-is-web-scrapping)

# ğŸ“˜ Day 22

## Python Web Scraping

### What is Web Scrapping


á¢áŸŠá¸á“á’áºáá·áá–áŸ„ášá–áŸá‰á‘áŸ…áŠáŸ„á™á‘á·á“áŸ’á“á“áŸá™áŠáŸá…áŸ’ášá¾á“ áŠáŸ‚á›á¢á¶á…á”áŸ’ášá¾á”áŸ’ášá¶áŸáŸ‹áŸá˜áŸ’ášá¶á”áŸ‹á‚áŸ„á›áŠáŸ…á•áŸ’áŸáŸá„áŸ— áŸ” áŠá¾á˜áŸ’á”á¸á”áŸ’ášá˜á¼á›á‘á·á“áŸ’á“á“áŸá™á“áŸáŸ‡á™á¾á„ááŸ’ášá¼áœáŠá¹á„á–á¸ášá”áŸ€á”ášá»á€ášá€ (scrape) á‘á·á“áŸ’á“á“áŸá™á–á¸á‚áŸá á‘áŸ†á–áŸáš áŸ”

á€á¶ášášá»á€ášá€áœáŸá”áŸá¶á™ á‚áºá‡á¶áŠáŸ†áá¾ášá€á¶ášá“áŸƒá€á¶ášá‘á¶á‰á™á€á“á·á„á”áŸ’ášá˜á¼á›á‘á·á“áŸ’á“á“áŸá™á–á¸á‚áŸá á‘áŸ†á–áŸáš á“á·á„ášá€áŸ’áŸá¶á‘á»á€áœá¶á“áŸ…á›á¾á˜áŸ‰á¶áŸáŸŠá¸á“á¬á€áŸ’á“á»á„á‘á·á“áŸ’á“á“áŸá™áŸ”

á“áŸ…á€áŸ’á“á»á„á•áŸ’á“áŸ‚á€á“áŸáŸ‡á™á¾á„á“á¹á„á”áŸ’ášá¾ beautifulsoup á“á·á„ request package áŠá¾á˜áŸ’á”á¸ášá»á€ášá€á‘á·á“áŸ’á“á“áŸá™áŸ” á”áŸ’ášá—áŸá‘á€á‰áŸ’á…á”áŸ‹áŠáŸ‚á›á™á¾á„á€áŸ†á–á»á„á”áŸ’ášá¾á‚áº beautifulsoup 4 áŸ”

áŠá¾á˜áŸ’á”á¸á…á¶á”áŸ‹á•áŸ’áŠá¾á˜ášá»á€ášá€á‚áŸá á‘áŸ†á–áŸáš á¢áŸ’á“á€ááŸ’ášá¼áœá€á¶áš _requests_, _beautifoulSoup4_ and a _website_.

```sh
pip install requests
pip install beautifulsoup4
```

áŠá¾á˜áŸ’á”á¸ášá»á€ášá€á‘á·á“áŸ’á“á“áŸá™á–á¸á‚áŸá á‘áŸ†á–áŸáš á€á¶ášá™á›áŸ‹áŠá¹á„á‡á¶á˜á¼á›áŠáŸ’á‹á¶á“á“áŸƒ HTML tags á“á·á„ CSS selectors ááŸ’ášá¼áœá€á¶ášáŸ” á™á¾á„á”á¶á“á…á¶á”áŸ‹á™á€á˜á¶áá·á€á¶á–á¸á‚áŸá á‘áŸ†á–áŸášá˜á½á™ áŠáŸ„á™á”áŸ’ášá¾ HTML tags, classes á¬/á“á·á„ ids áŸ”

áŸá¼á˜á™á¾á„á“á¶áŸ†á…á¼á› (import) requests á“á·á„ BeautifulSoup module

```py
import requests
from bs4 import BeautifulSoup
```

á™á¾á„ááŸ’ášá¼áœá”áŸ’ášá€á¶áŸáá¶ url variable áŸá˜áŸ’ášá¶á”áŸ‹ website áŠáŸ‚á›á™á¾á„á“á¹á„ášá»á€ášá€ áŸ”

```py

import requests
from bs4 import BeautifulSoup
url = 'https://archive.ics.uci.edu/ml/datasets.php'

# Lets use the requests get method to fetch the data from url

response = requests.get(url)
# lets check the status
status = response.status_code
print(status) # 200 means the fetching was successful
```

```sh
200
```

á”áŸ’ášá¾ beautifulSoup áŠá¾á˜áŸ’á”á¸á–á·á“á·ááŸ’á™á˜á¶áá·á€á¶á–á¸á‘áŸ†á–áŸáš

```py
import requests
from bs4 import BeautifulSoup
url = 'https://archive.ics.uci.edu/ml/datasets.php'

response = requests.get(url)
content = response.content # we get all the content from the website
soup = BeautifulSoup(content, 'html.parser') # beautiful soup will give a chance to parse
print(soup.title) # <title>UCI Machine Learning Repository: Data Sets</title>
print(soup.title.get_text()) # UCI Machine Learning Repository: Data Sets
print(soup.body) # gives the whole page on the website
print(response.status_code)

tables = soup.find_all('table', {'cellpadding':'3'})
# We are targeting the table with cellpadding attribute with the value of 3
# We can select using id, class or HTML tag , for more information check the beautifulsoup doc
table = tables[0] # the result is a list, we are taking out data from it
for td in table.find('tr').find_all('td'):
    print(td.text)
```

á”áŸ’ášáŸá·á“á”á¾á¢áŸ’á“á€ášááŸ‹á€á¼áŠá“áŸáŸ‡ á¢áŸ’á“á€á¢á¶á…á˜á¾á›áƒá¾á‰áá¶ á€á¶ášá‘á¶á‰á™á€á‚áºá”á¶á“á”á‰áŸ’á…á”áŸ‹á–á¶á€áŸ‹á€ááŸ’áá¶á› áŸ” á¢áŸ’á“á€á¢á¶á…á”á“áŸ’áá’áŸ’áœá¾áœá¶á”á¶á“
áŸá˜áŸ’ášá¶á”áŸ‹á€á¶ášááŸ‚á“á¶áŸ† áŸá¼á˜á˜á¾á› [beautifulsoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start)

ğŸ‰ CONGRATULATIONS ! ğŸ‰

[<< Day 21](../21_Day_Web_scraping/21_class_and_object.md) | [Day 23 >>](../23_Day_Virtual_environment/23_virtual_environment.md)
